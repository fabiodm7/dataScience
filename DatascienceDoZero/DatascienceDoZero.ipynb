{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encontrando conectores chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "(4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
    "\n",
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "\n",
    "for i,j in friendships:\n",
    "    users[i][\"friends\"].append(users[j])\n",
    "    users[j][\"friends\"].append(users[i])\n",
    "\n",
    "def number_of_friends(user):\n",
    "    \"\"\"quantos amigos o usuário tem?\"\"\"\n",
    "    return len(user[\"friends\"])\n",
    "\n",
    "total_connections = sum(number_of_friends(user) for user in users)\n",
    "\n",
    "num_users = len(users)\n",
    "avg_connections = total_connections // num_users\n",
    "\n",
    "# cria uma lista (user_id, number_of_friends)\n",
    "num_friends_by_id = [(user[\"id\"],number_of_friends(user)) for user in users]\n",
    "\n",
    "sorted(num_friends_by_id, key=lambda user_tuple: user_tuple[1], reverse=True)\n",
    "\n",
    "print(\"total_connections: \" + str(total_connections))\n",
    "print(\"num_users: \" + str(num_users))\n",
    "print(\"avg_connections: \" + str(avg_connections))\n",
    "print(\"num_friends_by_id: \" + str(num_friends_by_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cientistas de dados que você talvez conheça"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friends_of_friend_ids_bad(user):\n",
    "    # \"foaf\" é abreviação de \"friend of a friend\"\n",
    "    return [foaf[\"id\"]\n",
    "            for friend in user[\"friends\"]\n",
    "            for foaf in friend[\"friends\"]]\n",
    "\n",
    "print(friends_of_friend_ids_bad(users[0]))\n",
    "\n",
    "print([friend[\"id\"] for friend in users[0][\"friends\"]])\n",
    "print([friend[\"id\"] for friend in users[1][\"friends\"]])\n",
    "print([friend[\"id\"] for friend in users[2][\"friends\"]])\n",
    "\n",
    "from collections import Counter\n",
    "def not_the_same(user,other_user):\n",
    "    \"\"\"dois usuários não são os mesmo se possuem ids diferentes\"\"\"\n",
    "    return user[\"id\"] != other_user[\"id\"]\n",
    "\n",
    "def not_friends(user,other_user):\n",
    "    \"\"\"other_user não é um amigo se não está em user[\"friends];\n",
    "    isso é, se é not_the_same com todas as pessoas em user[\"friends\"]\"\"\"\n",
    "    return all(not_the_same(friend,other_user)\n",
    "               for friend in user[\"friends\"])\n",
    "\n",
    "def friends_of_friends_ids(user):\n",
    "    \"\"\"encontra o id para cada um dos meus amigos que contam\n",
    "    *their* amigos que não sejam eu e que não são meus amigos\"\"\"\n",
    "    return Counter(foaf[\"id\"]\n",
    "                   for friend in user[\"friends\"]\n",
    "                   for foaf in friend[\"friends\"]\n",
    "                   if not_the_same(user,foaf)\n",
    "                   and not_friends(user,foaf))\n",
    "\n",
    "print(friends_of_friends_ids(users[3]))\n",
    "\n",
    "interests = [\n",
    "    (0, \"Hadoop\"), (0, \"Big Data\"), (0, \"HBase\"), (0, \"Java\"),\n",
    "    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\"),\n",
    "    (1, \"NoSQL\"), (1, \"MongoDB\"), (1, \"Cassandra\"), (1, \"HBase\"),\n",
    "    (1, \"Postgres\"), (2, \"Python\"), (2, \"scikit-learn\"), (2, \"scipy\"),\n",
    "    (2, \"numpy\"), (2, \"statsmodels\"), (2, \"pandas\"), (3, \"R\"), (3, \"Python\"),\n",
    "    (3, \"statistics\"), (3, \"regression\"), (3, \"probability\"),\n",
    "    (4, \"machine learning\"), (4, \"regression\"), (4, \"decision trees\"),\n",
    "    (4, \"libsvm\"), (5, \"Python\"), (5, \"R\"), (5, \"Java\"), (5, \"C++\"),\n",
    "    (5, \"Haskell\"), (5, \"programming languages\"), (6, \"statistics\"),\n",
    "    (6, \"probability\"), (6, \"mathematics\"), (6, \"theory\"),\n",
    "    (7, \"machine learning\"), (7, \"scikit-learn\"), (7, \"Mahout\"),\n",
    "    (7, \"neural networks\"), (8, \"neural networks\"), (8, \"deep learning\"),\n",
    "    (8, \"Big Data\"), (8, \"artificial intelligence\"), (9, \"Hadoop\"),\n",
    "    (9, \"Java\"), (9, \"MapReduce\"), (9, \"Big Data\")\n",
    "]\n",
    "\n",
    "def data_scientists_who_like(target_interest):\n",
    "    return [user_id for user_id,user_interest in interests\n",
    "            if user_interest == target_interest]\n",
    "\n",
    "print(data_scientists_who_like(\"machine learning\"))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# as chave são interesses, os valores são listas de user_ids com interests\n",
    "user_ids_by_interest = defaultdict(list)\n",
    "for user_id,interest in interests:\n",
    "    user_ids_by_interest[interest].append(user_id)\n",
    "\n",
    "# as chaves são user_ids, os valores são as listas de interest para aquele user_id\n",
    "interests_by_user_id = defaultdict(list)\n",
    "for user_id,interest in interests:\n",
    "    interests_by_user_id[user_id].append(interest)\n",
    "\n",
    "def most_commom_interest_with(user):\n",
    "    \"\"\"itera sobre os interesses do usuário;\n",
    "    para cada interesse, itera sobre os outros usuários com aquele interesse;\n",
    "    mantém a cotagem de quantas vezes vemos cada outro usuário\"\"\"\n",
    "    return Counter(interested_user_id\n",
    "                   for interest in interests_by_user_id[user[\"id\"]]\n",
    "                   for interested_user_id in user_ids_by_interest[interest]\n",
    "                   if interested_user_id != user[\"id\"])\n",
    "\n",
    "print(most_commom_interest_with(users[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## salários e experiência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_and_tenures = [(83000, 8.7), (88000, 8.1),\n",
    "    (48000, 0.7), (76000, 6),\n",
    "    (69000, 6.5), (76000, 7.5),\n",
    "    (60000, 2.5), (83000, 10),\n",
    "    (48000, 1.9), (63000, 4.2)]\n",
    "\n",
    "# as chaves são os anos, os valores são as listas dos salários para cada ano\n",
    "salary_by_tenure = defaultdict(list)\n",
    "for salary,tenure in salaries_and_tenures:\n",
    "    salary_by_tenure[tenure].append(salary)\n",
    "\n",
    "# as chaves são os anos, cada valor é a média salarial para aquele ano\n",
    "average_salary_by_tenure = {\n",
    "    tenure : sum(salaries) / len(salaries)\n",
    "    for tenure,salaries in salary_by_tenure.items()\n",
    "}\n",
    "\n",
    "print(average_salary_by_tenure)\n",
    "\n",
    "def tenure_bucket(tenure):\n",
    "    \"\"\"salários agrupados em grupos\"\"\"\n",
    "    if tenure < 2:\n",
    "        return \"less than two\"\n",
    "    elif tenure < 5:\n",
    "        return \"between two and five\"\n",
    "    else:\n",
    "        return \"more than five\"\n",
    "    \n",
    "# as chaves são agrupamentos dos casos, os valores são as listas\n",
    "# dos salários para aquele agrupamento\n",
    "salary_by_tenure_bucket = defaultdict(list)\n",
    "for salary,tenure in salaries_and_tenures:\n",
    "    bucket = tenure_bucket(tenure)\n",
    "    salary_by_tenure_bucket[bucket].append(salary)\n",
    "\n",
    "# as chaves são agrupamentos dos casos, os valores são\n",
    "# a média salarial para aquele agrupamento\n",
    "average_salary_by_bucket = {\n",
    "    tenure_bucket : sum(salaries) / len(salaries)\n",
    "    for tenure_bucket,salaries in salary_by_tenure_bucket.items()\n",
    "}\n",
    "\n",
    "print(average_salary_by_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contas pagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7 paid\n",
    "# 1.9 unpaid\n",
    "# 2.5 paid\n",
    "# 4.2 unpaid\n",
    "# 6 unpaid\n",
    "# 6.5 unpaid\n",
    "# 7.5 unpaid\n",
    "# 8.1 unpaid\n",
    "# 8.7 paid\n",
    "# 10 paid\n",
    "\n",
    "def predict_paid_or_unpaid(years_experience):\n",
    "    if years_experience < 3.0:\n",
    "        return \"paid\"\n",
    "    elif years_experience < 8.5:\n",
    "        return \"unpaid\"\n",
    "    else:\n",
    "        return \"paid\"\n",
    "    \n",
    "print(predict_paid_or_unpaid(0.7))\n",
    "print(predict_paid_or_unpaid(5))\n",
    "print(predict_paid_or_unpaid(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tópicos de interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_and_counts = Counter(word\n",
    "                           for user, interest in interests\n",
    "                           for word in interest.lower().split())\n",
    "\n",
    "for word, count in words_and_counts.most_common():\n",
    "    if count > 1:\n",
    "        print(word,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. visualização de dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "years = [1950, 1960, 1970, 1980, 1990, 2000, 2010]\n",
    "gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]\n",
    "\n",
    "# cria um gráfico de linha, anos no eixo x, gdp no eixo y\n",
    "plt.plot(years,gdp,color='green',marker='o',linestyle='solid')\n",
    "\n",
    "# adiciona um título\n",
    "plt.title(\"GDP Nominal\")\n",
    "\n",
    "# adiciona um selo no eixo y\n",
    "plt.ylabel(\"Bilhões de $\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## gráfico de barras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = [\"Annie Hall\", \"Ben-Hur\", \"Casablanca\", \"Gandhi\", \"West Side Story\"]\n",
    "num_oscars = [5, 11, 3, 8, 10]\n",
    "\n",
    "# barras possuem o tamanho padrão de 0.8, então adicionaremos 0.1 às\n",
    "# coordenadas à esquerda para que cada barra seja centralizada\n",
    "xs = [i for i,_ in enumerate(movies)]\n",
    "\n",
    "# as barras do gráfico com as coordenadas x à esquerda [xs], alturas [num_oscars]\n",
    "plt.bar(xs,num_oscars)\n",
    "\n",
    "plt.ylabel(\"# de premiações\")\n",
    "plt.title(\"Meus filmes favoritos\")\n",
    "\n",
    "# nomeia o eixo x com nomes de filmes na barra central\n",
    "plt.xticks([i for i,_ in enumerate(movies)],movies)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = [83,95,91,87,70,0,85,82,100,67,73,77,0]\n",
    "decile = lambda grade: grade // 10*10\n",
    "histogram = Counter(decile(grade) for grade in grades)\n",
    "\n",
    "plt.bar([x for x in histogram.keys()], # move cada barra para a esquerda em 4\n",
    "        histogram.values(), # dá para cada barra sua altura correta\n",
    "        8) # dá para cada barra a largura de 8\n",
    "\n",
    "plt.axis([-5, 105, 0, 5]) # eixo x de -5 até 105,eixo y de 0 até 5\n",
    "\n",
    "plt.xticks([10 * i for i in range(11)]) # rótulos do eixo x em 0,10,...,100\n",
    "plt.xlabel(\"Decil\")\n",
    "plt.ylabel(\"# de alunos\")\n",
    "plt.title(\"Distribuição de notas do teste 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = [500, 505]\n",
    "years = [2013, 2014]\n",
    "\n",
    "plt.bar(years, mentions, 0.8)\n",
    "plt.xticks(years)\n",
    "\n",
    "plt.ylabel(\"# de vezes que ouvimos alguém dizer 'data science'\")\n",
    "\n",
    "# se você não fizer isso, matplotlib nomeará o eixo x de 0, 1\n",
    "# e então adiciona a +2.013e3 para fora do canto (matplotlib feio!)\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "\n",
    "# enganar o eixo y mostra apenas a parte acima de 500\n",
    "plt.axis([2012.5,2014.5,499,506])\n",
    "plt.title('Olhe o \"Grande\" Aumento!')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(years, mentions, 0.8)\n",
    "plt.xticks(years)\n",
    "\n",
    "plt.ylabel(\"# de vezes que ouvimos alguém dizer 'data science'\")\n",
    "\n",
    "# se você não fizer isso, matplotlib nomeará o eixo x de 0, 1\n",
    "# e então adiciona a +2.013e3 para fora do canto (matplotlib feio!)\n",
    "plt.ticklabel_format(useOffset=False)\n",
    "plt.axis([2012.5,2014.5,0,550])\n",
    "plt.title(\"Não Tão Grande Agora\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gráfico de linhas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]\n",
    "total_error = [x + y for x,y in zip(variance,bias_squared)]\n",
    "xs = [i for i,_ in enumerate(variance)]\n",
    "\n",
    "# podemos fazer múltiplas chamadas plt.plot\n",
    "# para mostrar múltiplas séries no mesmo gráfico\n",
    "plt.plot(xs,variance,'g-',label='variance') # linha verde sólida\n",
    "plt.plot(xs,bias_squared,'r-.',label='bias^2') # linha com linha de ponto tracejado vermelho\n",
    "plt.plot(xs,total_error,'b:',label='total error') # linha com pontilhado azul\n",
    "\n",
    "# porque atribuímos rótulos para cada série\n",
    "# podemos obter uma legenda gratuita\n",
    "# loc=9 significa \"top center\"\n",
    "plt.legend(loc=9)\n",
    "plt.xlabel(\"complexidade do modelo\")\n",
    "plt.title(\"compromisso entre polarização e variância\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## gráfico de dispersão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = [ 70, 65, 72, 63, 71, 64, 60, 64, 67]\n",
    "minutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n",
    "\n",
    "plt.scatter(friends,minutes)\n",
    "\n",
    "#nomeia cada posição\n",
    "for label,friend_count,minute_count in zip(labels,friends,minutes):\n",
    "    plt.annotate(label,\n",
    "                 xy=(friend_count,minute_count), # coloca o rótulo com sua posição\n",
    "                 xytext=(5,-5), # mas compensa um pouco\n",
    "                 textcoords='offset points')\n",
    "    \n",
    "plt.title(\"Minutos diários vs. número de amigos\")\n",
    "plt.xlabel('# de amigos')\n",
    "plt.ylabel('minutos diários passados no site')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_grades = [ 99, 90, 85, 97, 80]\n",
    "test_2_grades = [100, 85, 60, 90, 70]\n",
    "plt.scatter(test_1_grades, test_2_grades)\n",
    "plt.title(\"Os eixos não são compatíveis\")\n",
    "plt.xlabel(\"nota do teste 2\") \n",
    "plt.ylabel(\"nota do teste 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_grades = [ 99, 90, 85, 97, 80]\n",
    "test_2_grades = [100, 85, 60, 90, 70]\n",
    "plt.scatter(test_1_grades, test_2_grades)\n",
    "plt.title(\"Os eixos são compatíveis\")\n",
    "plt.xlabel(\"nota do teste 2\")\n",
    "plt.ylabel(\"nota do teste 1\")\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. algebra linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vetores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vector_add(v,w):\n",
    "    \"\"\"soma elementos correspondentes\"\"\"\n",
    "    return [v_i + w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "def vector_subtract(v,w):\n",
    "    \"\"\"subtrai elementos correspondentes\"\"\"\n",
    "    return [v_i - w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    \"\"\"soma elementos correspondentes\"\"\"\n",
    "    result = vectors[0]\n",
    "    for vector in vectors[1:]:\n",
    "        result = vector_add(result,vector)\n",
    "    return result\n",
    "\n",
    "from functools import reduce,partial\n",
    "def vector_sum(vectors):\n",
    "    return reduce(vector_add,vectors)\n",
    "\n",
    "# vector_sum = partial(reduce,vector_add)\n",
    "\n",
    "def scalar_multiply(c,v):\n",
    "    \"\"\"c é um número, v é um vetor\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"computar o vetor cujo i-ésimo elemento seja a média dos\n",
    "    i-ésimos elementos dos vetores inclusos\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n,vector_sum(vectors))\n",
    "\n",
    "def dot(v,w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i,w_i in zip(v,w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v,v)\n",
    "\n",
    "import math\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_distance(v,w):\n",
    "    \"\"\"(v_1 - w_1)**2 + ... + (v_n - w_n)**2\"\"\"\n",
    "    return sum_of_squares(vector_subtract(v,w))\n",
    "\n",
    "def distance(v,w):\n",
    "    return math.sqrt(squared_distance(v,w))\n",
    "\n",
    "def distance(v,w):\n",
    "    return magnitude(vector_subtract(v,w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## matrizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A possui duas linhas e três colunas\n",
    "A = [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "\n",
    "# B possui três linhas e duas colunas\n",
    "B = [[1, 2],\n",
    "     [3, 4],\n",
    "     [5, 6]]\n",
    "\n",
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows,num_cols\n",
    "\n",
    "def get_row(A,i):\n",
    "    return A[i]\n",
    "\n",
    "def get_column(A,j):\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def make_matrix(num_rows,num_cols,entry_fn):\n",
    "    \"\"\"retorna a matriz num_rows X num_cols\n",
    "    cuja entrada (i,j)th é entry_fn(i,j)\"\"\"\n",
    "    return [[entry_fn(i,j)\n",
    "             for j in range(num_cols)]\n",
    "             for i in range(num_rows)]\n",
    "\n",
    "def is_diagonal(i,j):\n",
    "    \"\"\"1's na diagonal, 0's no demais lugares\"\"\"\n",
    "    return 1 if i == j else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. estatística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## descrevendo um conjunto único de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_friends = [random.randint(1,100) for _ in range(204)]\n",
    "\n",
    "friend_counts = Counter(num_friends)\n",
    "xs = range(101) # o valor maior é 100\n",
    "ys = [friend_counts[x] for x in xs] # a altura é somente # de amigos\n",
    "plt.bar(xs,ys)\n",
    "plt.axis([0,101,0,25])\n",
    "plt.title('Histograma da contagem de amigos')\n",
    "plt.xlabel('# de amigos')\n",
    "plt.ylabel('# de pessoas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de pontos nos dados\n",
    "num_points = len(num_friends)\n",
    "print(num_points)\n",
    "\n",
    "# maiores e menos valores\n",
    "largest_value = max(num_friends)\n",
    "print(largest_value)\n",
    "smallest_value = min(num_friends)\n",
    "print(smallest_value)\n",
    "\n",
    "sorted_values = sorted(num_friends)\n",
    "smallest_value = sorted_values[0]\n",
    "print(smallest_value)\n",
    "second_smallest_value = sorted_values[1]\n",
    "print(second_smallest_value)\n",
    "second_largest_value = sorted_values[-2]\n",
    "print(second_largest_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tendências centrais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "mean(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(v):\n",
    "    \"\"\"encontra o valor mais ao meio de v\"\"\"\n",
    "    n = len(v)\n",
    "    sorted_v = sorted(v)\n",
    "    midpoint = n // 2\n",
    "    if n % 2 == 1:\n",
    "        # se for ímpar, retorna o valor do meio\n",
    "        return sorted_v[midpoint]\n",
    "    else:\n",
    "        # se for par, retorna a média dos valores do meio\n",
    "        lo = midpoint -1\n",
    "        hi = midpoint\n",
    "        return (sorted_v[lo]+sorted_v[hi])/2\n",
    "    \n",
    "median(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile(x,p):\n",
    "    \"\"\"retorna o valor percentual p-ésimo em x\"\"\"\n",
    "    p_index = int(p * len(x))\n",
    "    return sorted(x)[p_index]\n",
    "\n",
    "print(quantile(num_friends,0.10))\n",
    "print(quantile(num_friends,0.25))\n",
    "print(quantile(num_friends,0.75))\n",
    "print(quantile(num_friends,0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(x):\n",
    "    \"\"\"retorna uma lista, pode haver mais de uma moda\"\"\"\n",
    "    counts = Counter(x)\n",
    "    max_count = max(counts.values())\n",
    "    return [x_i for x_i, count in counts.items()\n",
    "            if count == max_count]\n",
    "\n",
    "mode(num_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dispersão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"amplitude\" já possui significado em Python, então usaremos um nome diferente\n",
    "def data_range(x):\n",
    "    return max(x) - min(x)\n",
    "\n",
    "data_range(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_mean(x):\n",
    "    \"\"\"desloca x ao subtrair sua média (então o resultado tem a média 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"presume que x tem ao menos dois elemetos\"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations)/(n-1)\n",
    "\n",
    "variance(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "standard_deviation(num_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interquantile_range(x):\n",
    "    return quantile(x,0.75) - quantile(x,0.25)\n",
    "\n",
    "interquantile_range(num_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_minutes = [random.randint(1,60*24) for _ in range(204)]\n",
    "\n",
    "def covariance(x,y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x),de_mean(y))/(n-1)\n",
    "\n",
    "def correlation(x,y):\n",
    "    stdev_x = standard_deviation(x)\n",
    "    stdev_y = standard_deviation(y)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(x,y)/stdev_x/stdev_y\n",
    "    else:\n",
    "        return 0 # se não houver amplitude a correlação é zero\n",
    "    \n",
    "outlier = num_friends.index(100) # índice do valor discrepante\n",
    "\n",
    "num_friends_good = [x\n",
    "                    for i,x in enumerate(num_friends)\n",
    "                    if i != outlier]\n",
    "\n",
    "daily_minutes_good = [x\n",
    "                    for i,x in enumerate(daily_minutes)\n",
    "                    if i != outlier]\n",
    "\n",
    "correlation(num_friends_good,daily_minutes_good)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_kid():\n",
    "    return random.choice([\"boy\",\"girl\"])\n",
    "\n",
    "both_girls = 0\n",
    "older_girl = 0\n",
    "either_girl = 0\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "for _ in range(10000):\n",
    "    younger = random_kid()\n",
    "    older = random_kid()\n",
    "    if older == 'girl':\n",
    "        older_girl += 1\n",
    "    if older == 'girl' and younger == 'girl':\n",
    "        both_girls += 1\n",
    "    if older == 'girl' or younger == 'girl':\n",
    "        either_girl += 1\n",
    "\n",
    "print('P(both|older): ', both_girls / older_girl)\n",
    "print('P(both|either): ', both_girls / either_girl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribuições contínuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_pdf(x):\n",
    "    return 1 if x >= 0 and x < 1 else 0\n",
    "\n",
    "def uniform_cdf(x):\n",
    "    \"\"\"retorna a probabilidade de uma variável aleatória uniforme ser <= x\"\"\"\n",
    "    if x < 0: return 0 # a aleatória uniforme nunca é menor do que 0\n",
    "    elif x < 1: return x # por exemplo P(x <= 0,4) = 0,4\n",
    "    else: return 1 # a aleatória uniforme sempre é menor do que 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a distribuição normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x,mu=0,sigma=1):\n",
    "    sqrt_two_pi = math.sqrt(2*math.pi)\n",
    "    return (math.exp(-(x-mu)**2/2/sigma**2)/(sqrt_two_pi*sigma))\n",
    "\n",
    "xs = [x/10.0 for x in range(-50,50)]\n",
    "plt.plot(xs,[normal_pdf(x,sigma=1) for x in xs],'-',label='mu=0, sigma=1')\n",
    "plt.plot(xs,[normal_pdf(x,sigma=2) for x in xs],'--',label='mu=0, sigma=2')\n",
    "plt.plot(xs,[normal_pdf(x,sigma=0.5) for x in xs],':',label='mu=0, sigma=0.5')\n",
    "plt.plot(xs,[normal_pdf(x,mu=-1) for x in xs],'-.',label='mu=-1, sigma=1')\n",
    "plt.legend()\n",
    "plt.title('Diversas funções de densidade de probabilidade normais')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cdf(x,mu=0,sigma=1):\n",
    "    return (1 + math.erf((x-mu)/math.sqrt(2)/sigma))/2\n",
    "\n",
    "xs = [x/10.0 for x in range(-50,50)]\n",
    "plt.plot(xs,[normal_cdf(x,sigma=1) for x in xs],'-',label='mu=0, sigma=1')\n",
    "plt.plot(xs,[normal_cdf(x,sigma=2) for x in xs],'--',label='mu=0, sigma=2')\n",
    "plt.plot(xs,[normal_cdf(x,sigma=0.5) for x in xs],':',label='mu=0, sigma=0.5')\n",
    "plt.plot(xs,[normal_cdf(x,mu=-1) for x in xs],'-.',label='mu=-1, sigma=1')\n",
    "plt.legend(loc=4) # bottom tight\n",
    "plt.title('Diversas funções de densidade de distribuição cumulativa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normal_cdf(p,mu=0,sigma=1,tolerance=0.00001):\n",
    "    \"\"\"encontra o inverso mais próximo usando a busca binária\"\"\"\n",
    "    # se não for padrão, computa o padrão e redimensiona\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p,tolerance=tolerance)\n",
    "    low_z,low_p = -10.0,0 # normal_cdf(-10) está (muito perto de) 0\n",
    "    hi_z,ho_p = 10.0,1 # normal_cdf(10) está (muito perto de ) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z)/2 # considera o ponto do meio o valor\n",
    "        mid_p = normal_cdf(mid_z) # função de distribuição cumulativa lá\n",
    "        if mid_p < p:\n",
    "            # o ponto do meio ainda está baixo, procura acima\n",
    "            low_z,low_p = mid_z,mid_p\n",
    "        elif mid_p > p:\n",
    "            # o ponto do meio ainda está alto, procura abaixo\n",
    "            hi_z,hi_p = mid_z,mid_p\n",
    "        else:\n",
    "            break\n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o teorema do limite central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_trial(p):\n",
    "    return 1 if random.random() < p else 0\n",
    "\n",
    "def binomial(n,p):\n",
    "    return sum(bernoulli_trial(p) for _ in range(n))\n",
    "\n",
    "def make_hist(p,n,num_points):\n",
    "    data = [binomial(n,p) for _ in range(num_points)]\n",
    "    # usa um gráfico de barras para exibir as amostras binomiais atuais\n",
    "    histogram = Counter(data)\n",
    "    plt.bar([x -0.4 for x in histogram.keys()],\n",
    "            [v/num_points for v in histogram.values()],\n",
    "            0.8,\n",
    "            color='0.75')\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(n*p*(1-p))\n",
    "    # usa um gráfico de linhas para exibir uma aproximação do normal\n",
    "    xs = range(min(data),max(data)+1)\n",
    "    ys = [normal_cdf(1 + 0.5,mu,sigma) - normal_cdf(1-0.5,mu,sigma)\n",
    "           for i in xs]\n",
    "    plt.plot(xs,ys)\n",
    "    plt.title('Distribuição binomial vs. Aproximação normal')\n",
    "    plt.show()\n",
    "\n",
    "make_hist(0.75,100,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. hipótese e inferência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## teste estatístico de hipótese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemplo: lançar uma moeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n,p):\n",
    "    \"\"\"encontra mi e sigma correspondendo ao Binomial(n,p)\"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1-p) * n)\n",
    "    return mu, sigma\n",
    "\n",
    "# o cdf normal é a probabilidade que a variável esteja abaixo de um limite\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# está acima do limite se não estiver abaixo\n",
    "def normal_probability_above(lo,mu=0,sigma=1):\n",
    "    return 1 - normal_cdf(lo,mu,sigma)\n",
    "\n",
    "# está entre se for menos do que hi, mas não menor do que lo\n",
    "def normal_probability_between(lo,hi,mu=0,sigma=1):\n",
    "    return normal_cdf(hi,mu,sigma)-normal_cdf(lo,mu,sigma)\n",
    "\n",
    "# está fora se não estiver entre\n",
    "def normal_probability_outside(lo,hi,mu=0,sigma=1):\n",
    "    return 1 - normal_probability_between(lo,hi,mu,sigma)\n",
    "\n",
    "def normal_upper_bound(probability,mu=0,sigma=1):\n",
    "    \"\"\"retorna z para que p(Z <= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(probability,mu,sigma)\n",
    "\n",
    "def normal_lower_bound(probability,mu=0,sigma=1):\n",
    "    \"\"\"retorna z para que p(Z >= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(1-probability,mu,sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability,mu=0,sigma=1):\n",
    "    \"\"\"retorna os limites simétricos (sobre a média)\n",
    "    que contêm a probabilidade específica\"\"\"\n",
    "    tail_probability = (1-probability)/2\n",
    "    # limite superior deveria ter tail_probability acima\n",
    "    upper_bound = normal_lower_bound(tail_probability,mu,sigma)\n",
    "    # limite inferior deveria ter tail_probability abaixo\n",
    "    lower_bound = normal_upper_bound(tail_probability,mu,sigma)\n",
    "    return lower_bound,upper_bound\n",
    "\n",
    "mu_0,sigma_0 = normal_approximation_to_binomial(1000,0.5)\n",
    "print(mu_0,sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_two_sided_bounds(0.95,mu_0,sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% dos limites baseados na premissa p é 0,5\n",
    "lo,hi = normal_two_sided_bounds(0.95,mu_0,sigma_0)\n",
    "\n",
    "# mi e sigma reais baseadas em p=0,55\n",
    "mu_1,sigma_1 = normal_approximation_to_binomial(1000,0.55)\n",
    "\n",
    "# um erro tipo 2 significa que falhamos ao rejeitar a hipótese nula\n",
    "# que acontecerá quando X ainda estiver em nosso intervalo original\n",
    "type_2_probability = normal_probability_between(lo,hi,mu_1,sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = normal_upper_bound(0.95,mu_0,sigma_0)\n",
    "print(hi)\n",
    "\n",
    "type_2_probability = normal_probability_below(hi,mu_1,sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_p_value(x,mu=0,sigma=1):\n",
    "    if x >= mu:\n",
    "        # se x for maior do que a média, a coroa será o que for maior do que x\n",
    "        return 2 * normal_probability_above(x,mu,sigma)\n",
    "    else:\n",
    "        # se x for menor do que a média, a coroa será o que for menor do que x\n",
    "        return 2 * normal_probability_below(x,mu,sigma)\n",
    "    \n",
    "two_sided_p_value(529.5,mu_0,sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 # contagem do # de caras\n",
    "                    for _ in range(1000)) # em 1000 lançamentos\n",
    "    if num_heads >= 530 or num_heads <= 470: # e contagem da frequência\n",
    "        extreme_value_count += 1 # que # é 'extrema'\n",
    "\n",
    "print(extreme_value_count/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sided_p_value(531.5,mu_0,sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_p_value = normal_probability_above\n",
    "lower_p_value = normal_probability_below\n",
    "\n",
    "print(upper_p_value(524.5,mu_0,sigma_0))\n",
    "print(upper_p_value(526.5,mu_0,sigma_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intervalos de confiança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1-p_hat)/1000)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_two_sided_bounds(0.95,mu,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = 540/1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\n",
    "print(sigma)\n",
    "print(normal_two_sided_bounds(0.95,mu,sigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"lança uma moeda 1000 vezes, True = cara, False = coroa\"\"\"\n",
    "    return [random.random() < 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    \"\"\"usando 5% dos níveis de significância\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_experiment() for _ in range(1000)]\n",
    "num_rejections = len([experiment\n",
    "                      for experiment in experiments\n",
    "                      if reject_fairness(experiment)])\n",
    "\n",
    "print(num_rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exemplo: executando um teste a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_parameters(N,n):\n",
    "    p = n/N\n",
    "    sigma = math.sqrt(p * (1-p)/N)\n",
    "    return p,sigma\n",
    "\n",
    "def a_b_test_statistic(N_A,n_A,N_B,n_B):\n",
    "    p_A,sigma_A = estimated_parameters(N_A,n_A)\n",
    "    p_B,sigma_B = estimated_parameters(N_B,n_B)\n",
    "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)\n",
    "\n",
    "z = a_b_test_statistic(1000,200,1000,180)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = a_b_test_statistic(1000,200,1000,150)\n",
    "print(z)\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inferência bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(alpha,beta):\n",
    "    \"\"\"uma constante normalizada para que a probabilidade total seja 1\"\"\"\n",
    "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x,alpha,beta):\n",
    "    if x < 0 or x > 1: # sem peso fora de [0,1]\n",
    "        return 0\n",
    "    return x**(alpha-1)*(1-x)**(beta-1)/B(alpha,beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a ideia por trás do gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computa a soma dos elementos ao quadrado em v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimando o gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h)-f(x))/h\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "derivative_estimate = partial(difference_quotient,square,h=0.00001)\n",
    "# planeja mostrar que são basicamente o mesmo\n",
    "x = range(-10,10)\n",
    "plt.title(\"Derivada real vs. estimativa\")\n",
    "plt.plot(x, [derivative(x_i) for x_i in x], 'rx', label='Real')       # vermelho 'x'\n",
    "plt.plot(x, [derivative_estimate(x_i) for x_i in x], 'b+', label='Estimativa')  # azul '+'\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f,v,i,h):\n",
    "    \"\"\"computa o i-ésimo quociente diferencial partical de f em v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0) # adiciona h ao elemento i-ésimo de v\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w)-f(v))/h\n",
    "\n",
    "def estimate_gradient(f,v,h=0.00001):\n",
    "    return [partial_difference_quotient(f,v,i,h) for i,_ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usando o gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v,direction,step_size):\n",
    "    \"\"\"move step_size na direção a partir de v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i,direction_i in zip(v,direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# escolhe um ponto inicial aleatório\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v) # computa o gradient em v\n",
    "    next_v = step(v,gradient,-0.01) # pega um passo gradiente negativo\n",
    "    if distance(next_v,v) < tolerance: # para se estivermos convergindo\n",
    "        break\n",
    "    v = next_v # continua se não estivermos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## escolhendo o tamanho do próximo passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"retorna um nova função que é igual a f,\n",
    "    exceto que ele exibe infinito como saída\n",
    "    toda vez que f produz um erro\"\"\"\n",
    "    def safe_f(*args,**kwargs):\n",
    "        try:\n",
    "            return f(*args,**kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## juntando tudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn,gradient_fn,theta_0,tolerance=0.000001):\n",
    "    \"\"\"usa o gradiente descendente para encontrar theta\n",
    "    que minimize a função alvo\"\"\"\n",
    "    step_sizes = [100,10,1,0.1,0.01,0.001,0.0001,0.00001]\n",
    "    theta = theta_0 # ajusta theta para o valor inicial\n",
    "    target_fn = safe(target_fn) # versão segura de target_fn\n",
    "    value = target_fn(theta) # valor que estamos minimizando\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta,gradient,-step_size)\n",
    "                       for step_size in step_sizes]\n",
    "        # escolhe aquele que minimiza a função de erro\n",
    "        next_theta = min(next_thetas,key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # para se estivermos \"convergindo\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta,value = next_theta,next_value\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"retorna uma função que, para qualquer entrada, x retorna -f(x)\"\"\"\n",
    "    return lambda *args,**kwargs: -f(*args,**kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"o mesmo quando f retorna uma lista de números\"\"\"\n",
    "    return lambda *args,**kwargs: [-y for y in f(*args,**kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn,gradient_fn,theta_0,tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradiente descendente estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"gerador retorna os elementos do dado em ordem aleatória\"\"\"\n",
    "    indexes = [i for i,_ in enumerate(data)] # cria uma lista de índices\n",
    "    random.shuffle(indexes) # os embaralha\n",
    "    for i in indexes: # retorna os dados naquela ordem\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn,gradient_fn,x,y,theta_0,alpha_0=0.01):\n",
    "    data = zip(x,y)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    min_theta,min_value = None,float('inf')\n",
    "    iterations_with_no_improvement = 0\n",
    "    # se formos até 100 iterações sem melhorias, paramos\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i,y_i,theta) for x_i,y_i in data)\n",
    "    if value < min_value:\n",
    "        # se achou um novo mínimo, lembre-se\n",
    "        # e volte para o tamanho do passo original\n",
    "        min_theta,min_value = theta,value\n",
    "        iterations_with_no_improvement = 0\n",
    "        alpha = alpha_0\n",
    "    else:\n",
    "        # do contrário, não estamos melhorando, portanto tente\n",
    "        # diminuir o tamanho do passo\n",
    "        iterations_with_no_improvement += 1\n",
    "        alpha *= 0.9\n",
    "    # e ande um passo gradiente para todos os pontos de dados\n",
    "    for x_i,y_i in in_random_order(data):\n",
    "        gradient_i = gradient_fn(x_i,y_i,theta)\n",
    "        theta = vector_subtract(theta,scalar_multiply(alpha,gradient_i))\n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn,gradient_fn,x,y,theta_0,alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x,y,theta_0,alpha_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. obtendo dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stdin e stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egrep.py\n",
    "import sys,re\n",
    "\n",
    "# sys.argv é a lista dos argumentos da linha de comando\n",
    "# sys.argv[0] é o nome do programa em si\n",
    "# sys.argv[1] será o regex especificado na linhas de comando\n",
    "regex = sys.argv[1]\n",
    "\n",
    "# para cada linha passada pelo script\n",
    "for line in sys.stdin:\n",
    "    # se combinar com o regex, escreva-o para o stdout\n",
    "    if re.search(regex,line):\n",
    "        sys.stdout.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_count.py\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "\n",
    "# print vai para o sys.stdout\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_words.py\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# passa o número de palavras como primeiro argumento\n",
    "try:\n",
    "    num_words = int(sys.argv[1])\n",
    "except:\n",
    "    print(\"usage: most_common_words.py num_words\")\n",
    "    sys.exit(1) # código de saída não-zero indica erro\n",
    "\n",
    "counter = Counter(word.lower() # palavras em minúsculas\n",
    "                  for line in sys.stdin\n",
    "                  for word in line.strip().split() # se separam por espaços\n",
    "                  if word) # pula as palavras vazias\n",
    "\n",
    "for word,count in counter.most_common(num_words):\n",
    "    sys.stdout.write(str(count))\n",
    "    sys.stdout.write(\"\\t\")\n",
    "    sys.stdout.write(word)\n",
    "    sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lendo arquivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o básico de arquivos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'r' significa somente leitura\n",
    "file_for_reading = open('reading_file.txt','r')\n",
    "\n",
    "# 'W' é escrever -- destruirá o arquivo se ele já existir!\n",
    "file_for_writing = open('writing_file.txt','w')\n",
    "\n",
    "# 'a' é anexar -- para adicionar ao final do arquivo\n",
    "file_for_appending = open('appending_file.txt','a')\n",
    "\n",
    "# não se esqueça de fechar os arquivos ao terminar\n",
    "file_for_writing.close()\n",
    "\n",
    "starts_with_hash = 0\n",
    "\n",
    "with open('input.txt','r') as file:\n",
    "    for line in file: # observa cada linha do arquivo\n",
    "        if re.match(\"^#\",line): # use um regex para ver se começa com '#'\n",
    "            starts_with_hash += 1 # se começar, adicione 1 à contagem\n",
    "\n",
    "def get_domain(email_address):\n",
    "    \"\"\"separa no '@' e retorna na última parte\"\"\"\n",
    "    return email_address.lower().split('@')[-1]\n",
    "\n",
    "with open('email_address.txt','r') as f:\n",
    "    doamin_counts = Counter(get_domain(line.strip())\n",
    "                            for line in f\n",
    "                            if '@' in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arquivos delimitados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('tab_delimited_stock_prices.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        # process(date, symbol, closing_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('colon_delimited_stock_prices.txt','r') as f:\n",
    "    reader = csv.DictReader(f,delimiter=':')\n",
    "    for row in reader:\n",
    "        date = row['date']\n",
    "        symbol = row['symbol']\n",
    "        closing_price = float(row['closing_price'])\n",
    "        # process(date,symbol,closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
    "\n",
    "with open('comma_delimited_stock_prices.txt','w') as f:\n",
    "    writer = csv.writer(f,delimiter=',')\n",
    "    for stock,price in today_prices.items():\n",
    "        writer.writerow([stock,price])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo dados da internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html e sua subsequente pesquisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('http://www.exemple.com').text\n",
    "soup = BeautifulSoup(html,'html5lib')\n",
    "\n",
    "first_paragraph = soup.find('p') # ou somente soup.p\n",
    "\n",
    "first_paragraph_text = soup.p.text\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "\n",
    "first_paragraph_id = soup.p['id'] # surge um KeyError se não tiver 'id'\n",
    "first_paragraph_id2 = soup.p.get('id') # surge None se não tiver 'id'\n",
    "\n",
    "all_paragraphs = soup.find_all('p') # ou apenas soup('p')\n",
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "important_paragraphs = soup('p',{'class':'important'})\n",
    "important_paragraphs2 = soup('p','important')\n",
    "important_paragraphs3 = [p for p in soup('p') if 'important' in p.get('class',[])]\n",
    "\n",
    "# atenção, vai retornar o mesmo span múltiplas vezes\n",
    "# se ele ficar dentro de múltiplos divs\n",
    "# seja mais esperto se esse for o caso\n",
    "spans_inside_divs = [span\n",
    "                     for div in soup('div') # para cada <div> na página\n",
    "                     for span in div('span')] # encontra cada <span> dentro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemplo: livros o'reilly sobre dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infelizmente o site da o'reilly tem dificultado este tipo de exercício\n",
    "\n",
    "url = \"https://www.oreilly.com/search/?q=data&rows=100\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "books = soup('article')\n",
    "tds = len(books)\n",
    "print(f\"Número de livros encontrados: {tds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_video(td):\n",
    "    \"\"\"é um vídeo se tiver exatamente um pricelabel, e se\n",
    "    o texto corrido dentro do pricelabel começar com 'Video\"\"\"\n",
    "    pricelabels = td('span', 'pricelabel')\n",
    "    return (len(pricelabels) == 1 and\n",
    "pricelabels[0].text.strip().startswith(\"Video\"))\n",
    "print(len([td for td in tds if not is_video(td)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_info(td):\n",
    "    \"\"\"dado uma marcação BeautifulSoup <td> representando um livro,\n",
    "    extrai os detalhes do livro e retorna um dict\"\"\"\n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "    \"data.do?sortby=publicationDate&page=\"\n",
    "books = []\n",
    "NUM_PAGES = 31 # na época da escrita deste livro, provavelmente mais agora\n",
    "for page_num in range(1, NUM_PAGES + 1):\n",
    "    print(\"souping page\", page_num, \",\", len(books), \" found so far\")\n",
    "    url = base_url + str(page_num)\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "    for td in soup('td', 'thumbtext'):\n",
    "        if not is_video(td):\n",
    "            books.append(book_info(td))\n",
    "    # agora seja um bom cidadão e respeite os robots.txt!\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(book):\n",
    "    \"\"\"book['date'] se parece com 'November 2014' então precisamos\n",
    "    dividir o espaço e então pegar o segundo pedaço\"\"\"\n",
    "    return int(book['date'].split()[1])\n",
    "\n",
    "year_counts = Counter(get_year(book) for book in books if get_year(book) <= 2024)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "year = sorted(year_counts)\n",
    "book_counts = [year_counts[year] for year in years]\n",
    "plt.plot(years,book_counts)\n",
    "plt.ylabel('# de livros de dados')\n",
    "plt.title('A áre de dados é grande')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## usando apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json e xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "\"author\" : \"Joel Grus\",\n",
    "\"publicationYear\" : 2014,\n",
    "\"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "deserialized = json.loads(serialized)\n",
    "if \"data science\" in deserialized[\"topics\"]:\n",
    "    print(deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = \"\"\"<Book>\n",
    "    <Title>Data Science Book</Title>\n",
    "    <Author>Joel Grus</Author>\n",
    "    <PublicationYear>2014</PublicationYear>\n",
    "    <Topics>\n",
    "        <Topic>data</Topic>\n",
    "        <Topic>science</Topic>\n",
    "        <Topic>data science</Topic>\n",
    "    </Topics>\n",
    "</Book>\"\"\"\n",
    "\n",
    "deserialized = BeautifulSoup(xml, 'html.parser')\n",
    "topics = [topic.text for topic in deserialized.find_all('Topic')]\n",
    "print(deserialized.find_all('Topic'))\n",
    "if \"data science\" in topics:\n",
    "    print(deserialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usando uma api não autenticada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo['created_at']) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "last_5_repositories = sorted(repos,\n",
    "                             key=lambda r:r['created_at'],\n",
    "                             reverse=True)[:5]\n",
    "\n",
    "last_5_languages = [repo[\"language\"]\n",
    "                    for repo in last_5_repositories]\n",
    "\n",
    "print(dates)\n",
    "print(month_counts)\n",
    "print(weekday_counts)\n",
    "print(last_5_repositories)\n",
    "print(last_5_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. trabalhando com dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explorando seus dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explorando dados unidimensionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize(point,bucket_size):\n",
    "    \"\"\"reduza o ponto para o próximo múltiplo mais baixo de buckte_size\"\"\"\n",
    "    return bucket_size * math.floor(point/bucket_size)\n",
    "\n",
    "def make_histogram(points,bucket_size):\n",
    "    \"\"\"agrupa os pontos e conta quantos em cada bucket\"\"\"\n",
    "    return Counter(bucketize(point,bucket_size) for point in points)\n",
    "\n",
    "def plot_histogram(points,bucket_size,title=''):\n",
    "    histogram = make_histogram(points,bucket_size)\n",
    "    plt.bar(histogram.keys(),histogram.values(),width=bucket_size)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# uniforme entre -100 e 100\n",
    "uniform = [200 * random.random() - 100 for _ in range(10000)]\n",
    "\n",
    "# distribuição normal com média 0, desvio padrão 57\n",
    "normal = [57 * inverse_normal_cdf(random.random()) for _ in range(10000)]\n",
    "\n",
    "plot_histogram(uniform,10,'histograma de uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(normal,10,'histograma normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### duas dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_normal():\n",
    "    \"\"\"retorna um desenho aleatório de uma distribiução normal padrão\"\"\"\n",
    "    return inverse_normal_cdf(random.random())\n",
    "\n",
    "xs = [random_normal() for _ in range(1000)]\n",
    "ys1 = [x + random_normal()/2 for x in xs]\n",
    "ys2 = [-x + random_normal()/2 for x in xs]\n",
    "\n",
    "plt.scatter(xs,ys1,marker='.',color='black',label='ys1')\n",
    "plt.scatter(xs,ys2,marker='.',color='gray',label='ys2')\n",
    "plt.xlabel('xs')\n",
    "plt.ylabel('ys')\n",
    "plt.legend(loc=9)\n",
    "plt.title('Distribuições conjuntas muito diferentes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation(xs,ys1))\n",
    "print(correlation(xs,ys2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### muitas dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(data):\n",
    "    \"\"\"retorna o num_columns x num_columns matrix cuja entrada \n",
    "    (i,j)-ésima é a correlação entre as colunas de dados i e j\"\"\"\n",
    "    _,num_columns = shape(data)\n",
    "    def matrix_entry(i,j):\n",
    "        return correlation(get_column(data,i),get_column(data,j))\n",
    "    return make_matrix(num_columns,num_columns,matrix_entry)\n",
    "\n",
    "_,num_columns = shape(data)\n",
    "fig,ax = plt.subplot(num_columns,num_columns)\n",
    "\n",
    "for i in range(num_columns):\n",
    "    for j in range(num_columns):\n",
    "        # dispersa a column_j no eixo x versus column_i no eixo y\n",
    "        if i != j: ax[i][j].scatter(get_column(data,j),get_column(data,i))\n",
    "        # a menos que i == j, em cujo caso mostra o nome da série\n",
    "        else: ax[i][j].annotate('série' + str(i),(0.5,0.5),\n",
    "                                xycoords='axes fraction',\n",
    "                                ha='center',va='center')\n",
    "        # então esconde as etiquetas dos eixos exceto\n",
    "        # os gráficos inferiores e da esquerda\n",
    "        if i < num_columns - 1: ax[i][j].xaxis.set_visible(False)\n",
    "        if j > 0: ax[i][j].yaxis.set_visible(False)\n",
    "\n",
    "# conserta as etiquetas inferiores à direita e superiores à esquerda\n",
    "# dos eixos, que está errado pois seus gráficos somenete possuem textos\n",
    "ax[-1][-1].set_xlim(ax[0][-1].get_xlim())\n",
    "ax[0][0].set_ylim(ax[0][1].get_ylim())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## limpando e transformando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(input_row,parsers):\n",
    "    \"\"\"dada uma lista de interpretadores (alguns podem ser None)\n",
    "    aplique o apropriado a cada elemento de input_row\"\"\"\n",
    "    return [parser(value) if parser is not None else value\n",
    "            for value,parser in zip(input_row,parsers)]\n",
    "\n",
    "def parse_rows_with(reader,parsers):\n",
    "    \"\"\"envolve um reader para aplicar os interpretadores em\n",
    "    cada uma de suas linhas\"\"\"\n",
    "    for row in reader:\n",
    "        yield parse_row(row,parsers)\n",
    "\n",
    "def try_or_none(f):\n",
    "    \"\"\"envolve f para retornar None se f levantar um exceção\n",
    "    presume que f leve apenas uma entrada\"\"\"\n",
    "    def f_or_none(x):\n",
    "        try: return f(x)\n",
    "        except: return None\n",
    "    return f_or_none\n",
    "\n",
    "def parse_row(input_row,parsers):\n",
    "    return [try_or_none(parser)(value) if parser is not None else value\n",
    "            for value,parser in zip(input_row,parsers)]\n",
    "\n",
    "import dateutil.parser\n",
    "import csv\n",
    "data = []\n",
    "with open('comma_delimited_stock_prices.csv','r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in parse_rows_with(reader,[dateutil.parser.parse,None,float]):\n",
    "        data.append(line)\n",
    "\n",
    "for row in data:\n",
    "    if any(x is None for x in row):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse_field(field_name,value,parser_dict):\n",
    "    \"\"\"tenta analisar o valor usando a função adequada \n",
    "    a partir de parser_dict\"\"\"\n",
    "    parser = parser_dict.get(field_name) # None se não tiver tal entrada\n",
    "    if parser is not None:\n",
    "        return try_or_none(parser)(value)\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def parse_dict(input_dict,parser_dict):\n",
    "    return {field_name: try_parse_field(field_name,value,parser_dict)\n",
    "            for field_name,value in input_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manipulando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_aapl_price = max(row['closing_price'] \n",
    "                     for row in data\n",
    "                     if row['symbol'] == 'AAPL')\n",
    "\n",
    "# agrupa linhas por símbolo\n",
    "by_symbol = defaultdict(list)\n",
    "for row in data:\n",
    "    by_symbol[row['symbol']].append(row)\n",
    "\n",
    "# usa a compreensão do dict para encontrar o max para cada símbolo\n",
    "max_price_by_symbol = {symbol : max(row['closing_price']\n",
    "                                    for row in grouped_rows)\n",
    "                                    for symbol,grouped_rows in by_symbol.items()}\n",
    "\n",
    "def picker(field_name):\n",
    "    \"\"\"retorna uma função que recolhe um campo de um dict\"\"\"\n",
    "    return lambda row:row[field_name]\n",
    "\n",
    "def pluck(field_name,rows):\n",
    "    \"\"\"transforma uma lista de dicts em um alista de valores field_name\"\"\"\n",
    "    return map(picker(field_name),rows)\n",
    "\n",
    "def group_by(grouper,rows,value_transform=None):\n",
    "    # a chave é a saída de grouper, o valor é uma lista de linhas\n",
    "    grouped = defaultdict(list)\n",
    "    for row in rows:\n",
    "        grouped[grouper(row)].append(row)\n",
    "    if value_transform in None:\n",
    "        return grouped\n",
    "    else:\n",
    "        return {key : value_transform(rows)\n",
    "                for key,rows in grouped.items()}\n",
    "    \n",
    "max_price_by_symbol = group_by(picker('symbol'),\n",
    "                               data,\n",
    "                               lambda rows:max(pluck('closing_price',rows)))\n",
    "\n",
    "def percent_price_chage(yesterday,today):\n",
    "    return today['closing_price'] / yesterday['closing_price'] - 1\n",
    "\n",
    "def day_over_day_changes(grouped_rows):\n",
    "    # organiza as linhas por data\n",
    "    ordered = sorted(grouped_rows,key=picker('date'))\n",
    "    \n",
    "    # compacta com uma compensação para ter pares de dias consecutivos\n",
    "    return [{'symbol':today['symbol'],\n",
    "             'date':today['date'],\n",
    "             'change':percent_price_chage(yesterday,today)}\n",
    "             for yesterday,today in zip(ordered,ordered[1:])]\n",
    "\n",
    "# a chave é symbol, o valor é uma change de dicts\n",
    "changes_by_symbol = group_by(picker('symbol'),data,day_over_day_changes)\n",
    "\n",
    "# coleta todas as changes de dicts para uma lista grande\n",
    "all_changes = [change\n",
    "               for changes in changes_by_symbol.values()\n",
    "               for change in changes]\n",
    "\n",
    "max(all_changes,key=picker('change'))\n",
    "min(all_changes,key=picker('change'))\n",
    "\n",
    "# para combinar as mudanças percentuais, adicionamos 1 a cada um,\n",
    "# os multiplicamos e subtraímos 1 por exemplo,se combinarmos +10%\n",
    "# e -20%, a mudança geral é (1+10%)*(1-20%)-1 = 1.1*.8 - 1 = -12%\n",
    "def combine_pct_changes(pct_change1,pct_change2):\n",
    "    return (1 + pct_change1) * (1 + pct_change2) - 1\n",
    "\n",
    "def overall_change(changes):\n",
    "    return reduce(combine_pct_changes,pluck('change',changes))\n",
    "\n",
    "overall_change_by_month = group_by(lambda row: row['date'].month,\n",
    "                                   all_changes,overall_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## redimensionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## redução da dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
